{
 "cells": [
  {
   "cell_type": "code",
   "id": "78ff0ee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T03:37:14.246747Z",
     "start_time": "2025-08-18T03:37:08.577788Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "a857ee58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T03:37:21.823305Z",
     "start_time": "2025-08-18T03:37:21.810306Z"
    }
   },
   "source": [
    "vocab_size=51200\n",
    "seq_len=144\n",
    "hidden_size=512\n",
    "batch_size=16"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "343fe13d",
   "metadata": {},
   "source": [
    "tokenizer分词器（调用bert）"
   ]
  },
  {
   "cell_type": "code",
   "id": "43c6de4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T03:37:27.522944Z",
     "start_time": "2025-08-18T03:37:26.099940Z"
    }
   },
   "source": [
    "from transformers import BertTokenizer\n",
    "bert_dir= \"D:\\code\\\\vlm-longtail\\pretrained\\\\bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_dir, truncation_side='right')\n",
    "tokenizer.add_special_tokens({\"bos_token\": \"[DEC]\"})\n",
    "tokenizer"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='D:\\code\\vlm-longtail\\pretrained\\bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[DEC]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "26739019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T03:37:29.914207Z",
     "start_time": "2025-08-18T03:37:29.878204Z"
    }
   },
   "source": [
    "tokens =tokenizer(\n",
    "            \"hello good moring\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=seq_len,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids\n",
    "tokens"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  7592,  2204, 22993,  3070,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "7fe118c4",
   "metadata": {},
   "source": [
    "# 位置编码和embedding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1265dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the positional encoding as described in the \"Attention is All You Need\" paper.\n",
    "    hidden_size: 模型的维度，必须是偶数\n",
    "    max_len: 预先计算的最大序列长度\n",
    "    dropout: Dropout层的比率\n",
    "    \"\"\"\n",
    "    # 位置编码的形状为 (max_len, hidden_size)\n",
    "    def __init__(self,hidden_size,max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        #创建一个底板tensor，作为填入位置编码的容器\n",
    "        pe = torch.zeros(max_len,hidden_size)\n",
    "        position = torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)\n",
    "        #shape = (max_len,1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0,hidden_size,2).float()*(-math.log(10000)/hidden_size))\n",
    "        #shape = (hidden_size/2,)\n",
    "\n",
    "\n",
    "        pe[:,0::2] = torch.sin(position*div_term)\n",
    "        pe[:,1::2] = torch.cos(position*div_term)\n",
    "        pe = pe.unsqueeze(0) #在第0维增加一个维度，变成 (1, max_len, hidden_size)\n",
    "\n",
    "        self.register_buffer('pe', pe)  #将pe注册为buffer，这样它不会被视为模型参数，但会被保存和加载\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: 输入的tensor，形状为 (batch_size, seq_len, hidden_size)\n",
    "        返回位置编码后的tensor，形状为 (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:,x.size(1),:]\n",
    "        return x  #应用dropout层\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0467fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "input_ids = torch.randint(0,vocab_size,(batch_size, seq_len))\n",
    "# input_ids\n",
    "embedding_output = embedding(input_ids)\n",
    "pe = PositionalEncoding(max_len=5000, hidden_size=hidden_size)\n",
    "output = pe(embedding_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedb0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_layer(nn.Module):\n",
    "    def __init__(self,vocab_size,hidden_size,dropout=0.1,max_len=5000,layernorm_eps=1e-12):\n",
    "        super(Embedding_layer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,hidden_size)\n",
    "        self.pe = PositionalEncoding(max_len=max_len, hidden_size=hidden_size)\n",
    "        self.dropout=nn.Dropout(p=dropout)\n",
    "        self.layernorm =nn.LayerNorm(hidden_size,eps=layernorm_eps)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        input_emb = self.embedding(input_ids)\n",
    "        input_emb = self.pe(input_emb)\n",
    "        input_emb = self.dropout(input_emb)\n",
    "        # input_emb = self.layernorm(input_emb)\n",
    "        return input_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dd4317",
   "metadata": {},
   "source": [
    "# 注意力机制"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6062bdca352a46fc"
  },
  {
   "cell_type": "code",
   "id": "bfe31edd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T15:55:18.909299Z",
     "start_time": "2025-08-18T15:55:18.889295Z"
    }
   },
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):    \n",
    "    \"\"\"\n",
    "        计算缩放点积注意力\n",
    "        Args:\n",
    "            q (torch.Tensor): 查询张量, 形状 (..., seq_len_q, d_k)\n",
    "            k (torch.Tensor): 键张量, 形状 (..., seq_len_k, d_k)\n",
    "            v (torch.Tensor): 值张量, 形状 (..., seq_len_v, d_v), seq_len_k == seq_len_v\n",
    "            mask (torch.Tensor, optional): 掩码张量, 形状 (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "        Returns:\n",
    "            torch.Tensor: 输出张量\n",
    "            torch.Tensor: 注意力权重\n",
    "    \"\"\"\n",
    "    d_k=q.size(-1)\n",
    "    score = torch.matmul(q,k.transpose(-2,-1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        \n",
    "        score = score.masked_fill(mask==0,-1e9)\n",
    "\n",
    "\n",
    "    p_attn = F.softmax(score,dim=-1)\n",
    "    output = torch.matmul(p_attn,v)\n",
    "\n",
    "    return output,p_attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,hidden_size,num_heads=8,dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size //num_heads\n",
    "\n",
    "        assert self.head_dim * num_heads == hidden_size\n",
    "\n",
    "        self.w_q = nn.Linear(hidden_size,hidden_size)\n",
    "        self.w_k = nn.Linear(hidden_size,hidden_size)\n",
    "        self.w_v = nn.Linear(hidden_size,hidden_size)\n",
    "        self.w_o = nn.Linear(hidden_size,hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        q = self.w_q(q).view(batch_size,-1,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        k = self.w_k(k).view(batch_size,-1,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        v = self.w_v(v).view(batch_size,-1,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        # [batch,num_heads,seq_len,head_dim]\n",
    "        \n",
    "        output, p_attn = scaled_dot_product_attention(q,k,v,mask)\n",
    "        \n",
    "        output = output.view(batch_size,-1,self.num_heads*self.head_dim)\n",
    "        output = self.w_o(output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "    "
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T15:55:12.879424Z",
     "start_time": "2025-08-18T15:55:12.688587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 实例化多头注意力模块\n",
    "mha = MultiHeadAttention(hidden_size=hidden_size, num_heads=8)\n",
    "\n",
    "# 创建假的输入数据 (通常在自注意力中, q, k, v是相同的)\n",
    "# 形状: (batch_size, seq_len, d_model)\n",
    "q = torch.randn(batch_size, seq_len, hidden_size)\n",
    "k = torch.randn(batch_size, seq_len, hidden_size)\n",
    "v = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "print(f\"输入张量的形状: {q.shape}\")\n",
    "\n",
    "# 前向传播\n",
    "output = mha(q, k, v, mask=None)\n",
    "\n",
    "print(f\"输出张量的形状: {output.shape}\")\n",
    "\n",
    "# 验证一下输出\n",
    "# 最终输出的维度应该和输入的维度完全一致\n",
    "assert output.shape == torch.Size([batch_size, seq_len, hidden_size])\n",
    "\n",
    "print(\"\\n多头注意力模块构建成功！\")"
   ],
   "id": "e99f31d91c5a4003",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入张量的形状: torch.Size([64, 144, 512])\n",
      "输出张量的形状: torch.Size([64, 144, 512])\n",
      "\n",
      "多头注意力模块构建成功！\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T14:26:04.837071Z",
     "start_time": "2025-08-18T14:26:04.821070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self,hidden_size,ffn_dim,dropout=0.1):\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.w_1 = nn.Linear(hidden_size,ffn_dim)\n",
    "        self.w_2 = nn.Linear(ffn_dim,hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = self.w_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.w_2(x)\n",
    "        return x"
   ],
   "id": "5d215501afefc859",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T14:26:22.943687Z",
     "start_time": "2025-08-18T14:26:22.921691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SublayerConnect(nn.Module):\n",
    "    def __init__(self,hidden_size,dropout=0.1):\n",
    "        super(SublayerConnect,self).__init__()\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x,sublayer):\n",
    "        residual_output = sublayer(x) + x\n",
    "        return self.norm(residual_output)"
   ],
   "id": "d73e0c0470244f9a",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T14:37:06.359184Z",
     "start_time": "2025-08-18T14:37:06.345183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,hidden_size):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(hidden_size=hidden_size)\n",
    "        self.ffn = PositionwiseFeedForward(hidden_size=hidden_size,ffn_dim=hidden_size*4)\n",
    "        self.sublayer = nn.ModuleList(SublayerConnect(hidden_size=hidden_size) for _ in range(2))\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x,lambda x:self.self_attn(x,x,x,mask))\n",
    "        x = self.sublayer[1](x , self.ffn)\n",
    "        return x\n",
    "    "
   ],
   "id": "ac57b8a4d2118010",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T15:22:49.901759Z",
     "start_time": "2025-08-18T15:22:49.887761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mask_attn = MultiHeadAttention(hidden_size=hidden_size)\n",
    "        self.ffn = PositionwiseFeedForward(hidden_size=hidden_size,ffn_dim=hidden_size*4)\n",
    "        self.cross_attn = MultiHeadAttention(hidden_size=hidden_size)\n",
    "        self.sublayer = nn.ModuleList(SublayerConnect(hidden_size=hidden_size) for _ in range(3))\n",
    "        \n",
    "    def forward(self,x,memory,src_mask,tgt_mask):\n",
    "        x = self.sublayer[0](x,lambda x : self.mask_attn(x,x,x,tgt_mask))\n",
    "        x = self.sublayer[1](x,lambda x : self.cross_attn(x,memory,memory,src_mask))\n",
    "        x = self.sublayer[2](x,self.ffn)\n",
    "        return x"
   ],
   "id": "6025dfde79e3bb42",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T15:42:09.797638Z",
     "start_time": "2025-08-18T15:42:09.776638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_src_mask(src, pad_idx):\n",
    "    \"\"\"\n",
    "    为源序列创建掩码\n",
    "    Args:\n",
    "        src (torch.Tensor): 源序列张量, 形状 (batch_size, src_len)\n",
    "        pad_idx (int): padding token的ID\n",
    "    Returns:\n",
    "        torch.Tensor: 源序列掩码, 形状 (batch_size, 1, 1, src_len)\n",
    "    \"\"\"\n",
    "    # 1. 创建基础掩码 (True/False tensor)\n",
    "    # 形状: (batch_size, src_len)\n",
    "    src_mask = (src != pad_idx)\n",
    "\n",
    "    # 2. 增加维度以适配多头注意力机制\n",
    "    # 形状: (batch_size, 1, 1, src_len)\n",
    "    return src_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "# --- 生成并检查 ---\n",
    "encoder_output  = torch.randint(0,vocab_size,(batch_size, 10))\n",
    "decoder_input = torch.randint(0,vocab_size,(batch_size, 20))\n",
    "pad_token_id = 1\n",
    "src_mask = make_src_mask(encoder_output, pad_token_id)\n",
    "\n",
    "def make_tgt_mask(tgt, pad_idx):\n",
    "    \"\"\"\n",
    "    为目标序列创建掩码\n",
    "    Args:\n",
    "        tgt (torch.Tensor): 目标序列张量, 形状 (batch_size, tgt_len)\n",
    "        pad_idx (int): padding token的ID\n",
    "    Returns:\n",
    "        torch.Tensor: 目标序列掩码, 形状 (batch_size, 1, tgt_len, tgt_len)\n",
    "    \"\"\"\n",
    "    batch_size, tgt_len = tgt.shape\n",
    "    device = tgt.device\n",
    "    \n",
    "    # 1. 创建padding掩码\n",
    "    # 形状: (batch_size, 1, 1, tgt_len)\n",
    "    tgt_pad_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    # 2. 创建look-ahead掩码\n",
    "    # 形状: (tgt_len, tgt_len)\n",
    "    lookahead_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=device)).bool()\n",
    "\n",
    "    # 3. 合并两个掩码\n",
    "    # tgt_pad_mask广播为 (batch_size, 1, 1, tgt_len)\n",
    "    # lookahead_mask广播为 (1, 1, tgt_len, tgt_len) -> (batch_size, 1, tgt_len, tgt_len)\n",
    "    tgt_mask = tgt_pad_mask & lookahead_mask\n",
    "    \n",
    "    return tgt_mask\n",
    "\n",
    "print(\"\\n--- 源序列掩码 (src_mask) ---\")\n",
    "print(\"最终形状:\", src_mask.shape) # torch.Size([2, 1, 1, 5])\n",
    "# print(\"第一个样本的掩码:\", src_mask[0]) # tensor([[[[True, True, True, True, True]]]])\n",
    "# print(\"第二个样本的掩码:\", src_mask[1]) # tensor([[[[True, True, False, False, False]]]])\n",
    "\n",
    "tgt_mask = make_tgt_mask(decoder_input, pad_token_id)\n",
    "\n",
    "print(\"\\n--- 目标序列掩码 (tgt_mask) ---\")\n",
    "print(\"最终形状:\", tgt_mask.shape) # torch.Size([2, 1, 6, 6])\n",
    "\n",
    "print(\"\\n第一个样本的掩码 (无padding):\")\n",
    "# print(tgt_mask[0].squeeze()) # .squeeze()是为了方便查看"
   ],
   "id": "b1ab419deae88264",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 源序列掩码 (src_mask) ---\n",
      "最终形状: torch.Size([64, 1, 1, 10])\n",
      "\n",
      "--- 目标序列掩码 (tgt_mask) ---\n",
      "最终形状: torch.Size([64, 1, 20, 20])\n",
      "\n",
      "第一个样本的掩码 (无padding):\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T15:54:49.457838Z",
     "start_time": "2025-08-18T15:54:49.360834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoder = DecoderLayer(hidden_size=hidden_size)\n",
    "x = torch.randn((batch_size, 20, hidden_size))\n",
    "memory = torch.randn((batch_size, 10, hidden_size))\n",
    "output = decoder(x,memory, src_mask=src_mask, tgt_mask=tgt_mask)"
   ],
   "id": "d5e1c4328d949da4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 20, 20])\n",
      "torch.Size([64, 1, 1, 10])\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import copy\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        \"\"\"\n",
    "        核心编码器，是N个相同层的堆栈\n",
    "        Args:\n",
    "            layer (EncoderLayer): 一个编码器层实例\n",
    "            N (int): 堆叠的层数\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        # 使用clones函数复制N个编码器层\n",
    "        self.layers = clones(layer, N)\n",
    "        # 再添加一个最终的层归一化\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        依次将输入和掩码传递给每个层\n",
    "        Args:\n",
    "            x (torch.Tensor): 输入张量, 形状 (batch_size, seq_len, d_model)\n",
    "            mask (torch.Tensor): 掩码\n",
    "        Returns:\n",
    "            torch.Tensor: 输出张量, 形状 (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ],
   "id": "cfeb226734a2be15"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
